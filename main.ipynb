{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "import math\n",
    "from PIL import Image\n",
    "from math import floor\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import os\n",
    "from scipy.ndimage.interpolation import rotate\n",
    "import types\n",
    "import numbers\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PRE_TRAINED_RESNET18 = 2\n",
    "RELOAD = True\n",
    "EPOCHS = 50\n",
    "PATIENCE = 5\n",
    "initial_lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_data = pd.read_csv(\"train_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_valid_split(dataframe, test_size = 0.25, shuffle = False, random_seed = 0):\n",
    "    \"\"\" Return a list of splitted indices from a DataSet.\n",
    "    Indices can be used with DataLoader to build a train and validation set.\n",
    "    \n",
    "    Arguments:\n",
    "        A Dataframe\n",
    "        A test_size, as a float between 0 and 1 (percentage split) or as an int (fixed number split)\n",
    "        Shuffling True or False\n",
    "        Random seed\n",
    "    \"\"\"\n",
    "    length = len(dataframe)\n",
    "    indices = list(range(1,length))\n",
    "    \n",
    "    if shuffle == True:\n",
    "        random.seed(random_seed)\n",
    "        random.shuffle(indices)\n",
    "    \n",
    "    if type(test_size) is float:\n",
    "        split = floor(test_size * length)\n",
    "    elif type(test_size) is int:\n",
    "        split = test_size\n",
    "    else:\n",
    "        raise ValueError('%s should be an int or a float' % str)\n",
    "    return indices[split:], indices[:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KaggleAmazonDataset(Dataset):\n",
    "    \"\"\"Dataset wrapping images and target labels for Kaggle - Planet Amazon from Space competition.\n",
    "\n",
    "    Arguments:\n",
    "        A CSV file path\n",
    "        Path to image folder\n",
    "        Extension of images\n",
    "        PIL transforms\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_path, img_path, img_ext, transform=None):\n",
    "    \n",
    "        tmp_df = pd.read_csv(csv_path)\n",
    "        assert tmp_df['image_name'].apply(lambda x: os.path.isfile(img_path + x + img_ext)).all(), \\\n",
    "\"Some images referenced in the CSV file were not found\"\n",
    "        \n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "        self.img_path = img_path\n",
    "        self.img_ext = img_ext\n",
    "        self.transform = transform\n",
    "\n",
    "        self.X_train = tmp_df['image_name']\n",
    "        self.y_train = self.mlb.fit_transform(tmp_df['tags'].str.split()).astype(np.float32)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.img_path + self.X_train[index] + self.img_ext)\n",
    "        img = img.convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img).cuda()\n",
    "        \n",
    "        label = torch.from_numpy(self.y_train[index]).cuda()\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Lambda(object):\n",
    "    \"\"\"Apply a user-defined lambda as a transform.\n",
    "\n",
    "    Args:\n",
    "        lambd (function): Lambda/function to be used for transform.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lambd):\n",
    "        assert isinstance(lambd, types.LambdaType)\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return self.lambd(img)\n",
    "    \n",
    "    \n",
    "class RandomRotation(object):\n",
    "    \"\"\"Rotate the image by angle.\n",
    "\n",
    "    Args:\n",
    "        degrees (sequence or float or int): Range of degrees to select from.\n",
    "            If degrees is a number instead of sequence like (min, max), the range of degrees\n",
    "            will be (-degrees, +degrees).\n",
    "        resample ({PIL.Image.NEAREST, PIL.Image.BILINEAR, PIL.Image.BICUBIC}, optional):\n",
    "            An optional resampling filter.\n",
    "            See http://pillow.readthedocs.io/en/3.4.x/handbook/concepts.html#filters\n",
    "            If omitted, or if the image has mode \"1\" or \"P\", it is set to PIL.Image.NEAREST.\n",
    "        expand (bool, optional): Optional expansion flag.\n",
    "            If true, expands the output to make it large enough to hold the entire rotated image.\n",
    "            If false or omitted, make the output image the same size as the input image.\n",
    "            Note that the expand flag assumes rotation around the center and no translation.\n",
    "        center (2-tuple, optional): Optional center of rotation.\n",
    "            Origin is the upper left corner.\n",
    "            Default is the center of the image.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, degrees, resample=False, expand=False, center=None):\n",
    "        if isinstance(degrees, numbers.Number):\n",
    "            if degrees < 0:\n",
    "                raise ValueError(\"If degrees is a single number, it must be positive.\")\n",
    "            self.degrees = (-degrees, degrees)\n",
    "        else:\n",
    "            if len(degrees) != 2:\n",
    "                raise ValueError(\"If degrees is a sequence, it must be of len 2.\")\n",
    "            self.degrees = degrees\n",
    "\n",
    "        self.resample = resample\n",
    "        self.expand = expand\n",
    "        self.center = center\n",
    "\n",
    "    @staticmethod\n",
    "    def get_params(degrees):\n",
    "        \"\"\"Get parameters for ``rotate`` for a random rotation.\n",
    "\n",
    "        Returns:\n",
    "            sequence: params to be passed to ``rotate`` for random rotation.\n",
    "        \"\"\"\n",
    "        angle = np.random.uniform(degrees[0], degrees[1])\n",
    "\n",
    "        return angle\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "            img (PIL Image): Image to be rotated.\n",
    "\n",
    "        Returns:\n",
    "            PIL Image: Rotated image.\n",
    "        \"\"\"\n",
    "\n",
    "        angle = self.get_params(self.degrees)\n",
    "\n",
    "        return rotate(img, angle, reshape=False)\n",
    "\n",
    "class RandomVerticalFlip(object):\n",
    "    \"\"\"Vertically flip the given PIL Image randomly with a probability of 0.5.\"\"\"\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image): Image to be flipped.\n",
    "\n",
    "        Returns:\n",
    "            PIL Image: Randomly flipped image.\n",
    "        \"\"\"\n",
    "        if random.random() < 0.5:\n",
    "            return np.flip(img,1)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diego/.conda/envs/pytorch/lib/python3.6/site-packages/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "IMG_PATH = '/media/train-jpg/'\n",
    "IMG_EXT = '.jpg'\n",
    "TRAIN_DATA = 'train_v2.csv'\n",
    "\n",
    "# Data augmentation\n",
    "ds_transform_augmented = transforms.Compose([\n",
    "                 #transforms.RandomSizedCrop(224),\n",
    "                 transforms.Scale(224),\n",
    "                 transforms.RandomHorizontalFlip(),\n",
    "                 RandomVerticalFlip(),\n",
    "                 RandomRotation(10),\n",
    "                 #ColorJitter(0.5,0.5,0.5,0.5),\n",
    "                 transforms.ToTensor(),\n",
    "                 transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "                ])\n",
    "\n",
    "## Normalization only for validation and test\n",
    "ds_transform_raw = transforms.Compose([\n",
    "                 transforms.Scale(224),\n",
    "                 transforms.ToTensor(),\n",
    "                 transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.02, 0.02, 0.225])\n",
    "                 ])\n",
    "\n",
    "\n",
    "\n",
    "X_train = KaggleAmazonDataset(TRAIN_DATA,IMG_PATH,IMG_EXT,\n",
    "                             ds_transform_augmented\n",
    "                             )\n",
    "#X_val = KaggleAmazonDataset(TRAIN_DATA,IMG_PATH,IMG_EXT,\n",
    "#                             ds_transform_raw\n",
    "#                             )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating a validation split\n",
    "train_idx, valid_idx = train_valid_split(X_train, 0.2)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = DataLoader(X_train,\n",
    "                          batch_size=45,\n",
    "                          sampler=train_sampler,\n",
    "                          #num_workers=4,\n",
    "                          #pin_memory= True\n",
    "                         )\n",
    "\n",
    "valid_loader = DataLoader(X_train,\n",
    "                          batch_size=45,\n",
    "                          sampler=valid_sampler,\n",
    "                          #num_workers=4,\n",
    "                         #pin_memory= True\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adaptive_avgmax_pool2d(x, pool_type='avg', padding=0, count_include_pad=False):\n",
    "    \"\"\"Selectable global pooling function with dynamic input kernel size\n",
    "    \"\"\"\n",
    "    if pool_type == 'avgmaxc':\n",
    "        x = torch.cat([\n",
    "            F.avg_pool2d(\n",
    "                x, kernel_size=(x.size(2), x.size(3)), padding=padding, count_include_pad=count_include_pad),\n",
    "            F.max_pool2d(x, kernel_size=(x.size(2), x.size(3)), padding=padding)\n",
    "        ], dim=1)\n",
    "    elif pool_type == 'avgmax':\n",
    "        x_avg = F.avg_pool2d(\n",
    "                x, kernel_size=(x.size(2), x.size(3)), padding=padding, count_include_pad=count_include_pad)\n",
    "        x_max = F.max_pool2d(x, kernel_size=(x.size(2), x.size(3)), padding=padding)\n",
    "        x = 0.5 * (x_avg + x_max)\n",
    "    elif pool_type == 'max':\n",
    "        x = F.max_pool2d(x, kernel_size=(x.size(2), x.size(3)), padding=padding)\n",
    "    else:\n",
    "        if pool_type != 'avg':\n",
    "            print('Invalid pool type %s specified. Defaulting to average pooling.' % pool_type)\n",
    "        x = F.avg_pool2d(\n",
    "            x, kernel_size=(x.size(2), x.size(3)), padding=padding, count_include_pad=count_include_pad)\n",
    "    return x\n",
    "\n",
    "class AdaptiveAvgMaxPool2d(torch.nn.Module):\n",
    "    \"\"\"Selectable global pooling layer with dynamic input kernel size\n",
    "    \"\"\"\n",
    "    def __init__(self, output_size=1, pool_type='avg'):\n",
    "        super(AdaptiveAvgMaxPool2d, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.pool_type = pool_type\n",
    "        if pool_type == 'avgmaxc' or pool_type == 'avgmax':\n",
    "            self.pool = nn.ModuleList([nn.AdaptiveAvgPool2d(output_size), nn.AdaptiveMaxPool2d(output_size)])\n",
    "        elif pool_type == 'max':\n",
    "            self.pool = nn.AdaptiveMaxPool2d(output_size)\n",
    "        else:\n",
    "            if pool_type != 'avg':\n",
    "                print('Invalid pool type %s specified. Defaulting to average pooling.' % pool_type)\n",
    "            self.pool = nn.AdaptiveAvgPool2d(output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.pool_type == 'avgmaxc':\n",
    "            x = torch.cat([p(x) for p in self.pool], dim=1)\n",
    "        elif self.pool_type == 'avgmax':\n",
    "            x = 0.5 * torch.sum(torch.stack([p(x) for p in self.pool]), 0).squeeze(dim=0)\n",
    "        else:\n",
    "            x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "    def factor(self):\n",
    "        return pooling_factor(self.pool_type)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + 'output_size=' + str(self.output_size) \\\n",
    "+ ', pool_type=' + self.pool_type + ')'\n",
    "\n",
    "def pooling_factor(pool_type='avg'):\n",
    "    return 2 if pool_type == 'avgmaxc' else 1\n",
    "\n",
    "__all__ = ['ResNet', 'resnet18']\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth'\n",
    "}\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(\n",
    "        in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, drop_rate=0.0):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        self.drop_rate = drop_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        if self.drop_rate > 0.:\n",
    "            out = F.dropout(out, p=self.drop_rate, training=self.training)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, drop_rate=0.0):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        self.drop_rate = drop_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        if self.drop_rate > 0.:\n",
    "            out = F.dropout(out, p=self.drop_rate, training=self.training)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000,\n",
    "                 drop_rate=0.0, block_drop_rate=0.0,\n",
    "                 global_pool='avg'):\n",
    "        self.inplanes = 64\n",
    "        self.drop_rate = drop_rate\n",
    "        self.expansion = block.expansion\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0], drop_rate=block_drop_rate)\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, drop_rate=block_drop_rate)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, drop_rate=block_drop_rate)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, drop_rate=block_drop_rate)\n",
    "        self.global_pool = AdaptiveAvgMaxPool2d(pool_type=global_pool)\n",
    "        self.fc = nn.Linear(512 * block.expansion * self.global_pool.factor(), num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, drop_rate=0.):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = [block(self.inplanes, planes, stride, downsample, drop_rate)]\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def get_fc(self):\n",
    "        return self.fc\n",
    "\n",
    "    def reset_fc(self, num_classes, global_pool='avg'):\n",
    "        self.global_pool = AdaptiveAvgMaxPool2d(pool_type=global_pool)\n",
    "        self.fc = nn.Linear(512 * self.expansion * self.global_pool.factor(), num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        if self.drop_rate > 0.:\n",
    "            x = F.dropout(x, p=self.drop_rate, training=self.training)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def resnet18(pretrained=True, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n",
    "    return model\n",
    "\n",
    "    \n",
    "def resnet18_fc(num_classes, pretrained, embedding_len, bn, dp=False):\n",
    "    \"resnet18 + FC layer embedding\"\n",
    "    model = resnet18(num_classes)\n",
    "    if pretrained:\n",
    "        model = torch.nn.DataParallel(model)  # due to saving parallel table\n",
    "        checkpoint = torch.load(PRE_TRAINED_RESNET18)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        model = model.module\n",
    "\n",
    "    model.fc1 = nn.Linear(512, embedding_len)\n",
    "    model.relu_fc1 = nn.ReLU(inplace=True) #new\n",
    "    model.fc = nn.Linear(embedding_len, num_classes)\n",
    "    if bn:\n",
    "        model.fc_bn = nn.BatchNorm1d(embedding_len)\n",
    "\n",
    "    if dp:\n",
    "        model.dp = nn.Dropout2d(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        if x.size(2) != 1:\n",
    "            x = F.avg_pool2d(x, x.size(2))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu_fc1(self.fc1(x))\n",
    "\n",
    "        if getattr(self, 'fc_bn', None) is not None:\n",
    "            x = self.fc_bn(x)\n",
    "        if getattr(self, 'dp', None) is not None:\n",
    "            x = self.dp(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    setattr(model.__class__, 'forward', forward)\n",
    "\n",
    "    layers = [model.fc1, model.fc, model.avgpool, model.layer4, model.layer3]\n",
    "\n",
    "    return model, layers\n",
    "\n",
    "def resnet18_fc128(num_classes=17, pretrained=False, bn=False):\n",
    "    \"0.08693 loss, LB: 0.92047\"\n",
    "    return resnet18_fc(num_classes, pretrained, embedding_len=128, bn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLabelSoftMarginLoss (\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = resnet18(pretrained=True)\n",
    "model.reset_fc(17)\n",
    "optimizer = optim.SGD(model.get_fc().parameters(), lr=initial_lr, momentum=0.9)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=initial_lr, momentum=0.9)\n",
    "criterion = nn.MultiLabelSoftMarginLoss()\n",
    "criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "best_accuracy = 0.0\n",
    "current_patience = PATIENCE\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    return fbeta_score(output, target, beta=2, average=\"samples\")\n",
    "\n",
    "def train_generator(model,epoch,train_loader,optimizer,criterion):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.cuda(async=True), target.cuda(async=True) # On GPU\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 1000 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "                \n",
    "def test_generator(model, val_loader, criterion):\n",
    "        global current_patience\n",
    "        global best_accuracy\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        ammount = 0\n",
    "        equal = 0\n",
    "        pred_all = None\n",
    "        test_all = None\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.cuda(async=True), target.cuda(async=True) # On GPU\n",
    "            data, target = Variable(data.float(), volatile=True), Variable(target.float())\n",
    "            output = model(data)\n",
    "            ammount += len(data)\n",
    "            test_loss += criterion(output, target).data[0]\n",
    "            pred = output\n",
    "            #print(pred)\n",
    "            pred[pred > 0.1] = 1\n",
    "            pred[pred != 1] = 0\n",
    "\n",
    "            if type(pred_all) == type(None):\n",
    "                pred_all = pred.cpu().data.numpy().astype(np.uint8)\n",
    "            else:\n",
    "                pred = pred.cpu().data.numpy().astype(np.uint8)\n",
    "\n",
    "                pred_all = np.concatenate((pred_all,pred),axis=0)\n",
    "            if type(test_all) == type(None):\n",
    "                test_all = target.cpu().data.numpy().astype(np.uint8)\n",
    "            else:\n",
    "                target = target.cpu().data.numpy().astype(np.uint8)\n",
    "                test_all = np.concatenate((test_all,target),axis=0)\n",
    "            #accuracy(output, target,topk=(1,3))\n",
    "        accur = accuracy(pred_all,test_all)\n",
    "        \n",
    "        # Save the model or early stop\n",
    "        if accur > best_accuracy:\n",
    "            torch.save(model.state_dict(), \"resnet18-%s.pth\" % int(accur*100))\n",
    "            best_accuracy = accur\n",
    "            current_patience = PATIENCE\n",
    "        else:\n",
    "            if current_patience < 1:\n",
    "                return False\n",
    "            else:\n",
    "                current_patience -= 1\n",
    "            \n",
    "        test_loss /= ammount\n",
    "        print('Test set: Accuracy: %s  /  Patience: %s\\n' % (accur,current_patience))\n",
    "        \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/40479 (0%)]\tLoss: 0.175284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diego/.conda/envs/pytorch/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Accuracy: 0.857538820492  /  Patient: 10\n",
      "\n",
      "Train Epoch: 2 [0/40479 (0%)]\tLoss: 0.202931\n",
      "Test set: Accuracy: 0.861822648265  /  Patient: 10\n",
      "\n",
      "Train Epoch: 3 [0/40479 (0%)]\tLoss: 0.137516\n",
      "Test set: Accuracy: 0.866240191097  /  Patient: 10\n",
      "\n",
      "Train Epoch: 4 [0/40479 (0%)]\tLoss: 0.139011\n",
      "Test set: Accuracy: 0.868948904385  /  Patient: 10\n",
      "\n",
      "Train Epoch: 5 [0/40479 (0%)]\tLoss: 0.152866\n",
      "Test set: Accuracy: 0.869432413774  /  Patient: 10\n",
      "\n",
      "Train Epoch: 6 [0/40479 (0%)]\tLoss: 0.139370\n",
      "Test set: Accuracy: 0.872955622414  /  Patient: 10\n",
      "\n",
      "Train Epoch: 7 [0/40479 (0%)]\tLoss: 0.152728\n",
      "Test set: Accuracy: 0.876523588113  /  Patient: 10\n",
      "\n",
      "Train Epoch: 8 [0/40479 (0%)]\tLoss: 0.156205\n",
      "Test set: Accuracy: 0.878101374097  /  Patient: 10\n",
      "\n",
      "Train Epoch: 9 [0/40479 (0%)]\tLoss: 0.145416\n",
      "Test set: Accuracy: 0.879214698624  /  Patient: 10\n",
      "\n",
      "Train Epoch: 10 [0/40479 (0%)]\tLoss: 0.157576\n",
      "Test set: Accuracy: 0.882072277752  /  Patient: 10\n",
      "\n",
      "Train Epoch: 11 [0/40479 (0%)]\tLoss: 0.110819\n",
      "Test set: Accuracy: 0.882061965525  /  Patient: 9\n",
      "\n",
      "Train Epoch: 12 [0/40479 (0%)]\tLoss: 0.185914\n",
      "Test set: Accuracy: 0.88445147011  /  Patient: 9\n",
      "\n",
      "Train Epoch: 13 [0/40479 (0%)]\tLoss: 0.125891\n",
      "Test set: Accuracy: 0.885401023465  /  Patient: 9\n",
      "\n",
      "Train Epoch: 14 [0/40479 (0%)]\tLoss: 0.170169\n",
      "Test set: Accuracy: 0.884973724355  /  Patient: 8\n",
      "\n",
      "Train Epoch: 15 [0/40479 (0%)]\tLoss: 0.094474\n",
      "Test set: Accuracy: 0.888152068876  /  Patient: 8\n",
      "\n",
      "Train Epoch: 16 [0/40479 (0%)]\tLoss: 0.140945\n",
      "Test set: Accuracy: 0.890128854801  /  Patient: 8\n",
      "\n",
      "Train Epoch: 17 [0/40479 (0%)]\tLoss: 0.148125\n",
      "Test set: Accuracy: 0.89211430034  /  Patient: 8\n",
      "\n",
      "Train Epoch: 18 [0/40479 (0%)]\tLoss: 0.103184\n",
      "Test set: Accuracy: 0.892046447124  /  Patient: 7\n",
      "\n",
      "Train Epoch: 19 [0/40479 (0%)]\tLoss: 0.124240\n",
      "Test set: Accuracy: 0.894673664614  /  Patient: 7\n",
      "\n",
      "Train Epoch: 20 [0/40479 (0%)]\tLoss: 0.095329\n",
      "Test set: Accuracy: 0.895043317226  /  Patient: 7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.DataParallel(model).cuda()\n",
    "EPOCHS = 20\n",
    "#if RELOAD:\n",
    "    #model.load_state_dict(torch.load(\"resnet18-85.pth\"))\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    \n",
    "    train_generator(model,epoch,train_loader,optimizer,criterion)\n",
    "    \n",
    "    if not test_generator(model,valid_loader,criterion):\n",
    "        break\n",
    "        \n",
    "    # Adjust lr\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr']=initial_lr * (0.1 ** (epoch // EPOCHS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = torch.nn.DataParallel(model)\n",
    "#model.load_state_dict(torch.load(\"resnet18-85.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/40479 (0%)]\tLoss: 0.144004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diego/.conda/envs/pytorch/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Accuracy: 0.894796266754  /  Patient: 6\n",
      "\n",
      "Train Epoch: 2 [0/40479 (0%)]\tLoss: 0.086065\n",
      "Test set: Accuracy: 0.896112923869  /  Patient: 6\n",
      "\n",
      "Train Epoch: 3 [0/40479 (0%)]\tLoss: 0.139507\n",
      "Test set: Accuracy: 0.895621994201  /  Patient: 5\n",
      "\n",
      "Train Epoch: 4 [0/40479 (0%)]\tLoss: 0.128158\n",
      "Test set: Accuracy: 0.8968407134  /  Patient: 5\n",
      "\n",
      "Train Epoch: 5 [0/40479 (0%)]\tLoss: 0.138347\n",
      "Test set: Accuracy: 0.898827541927  /  Patient: 5\n",
      "\n",
      "Train Epoch: 6 [0/40479 (0%)]\tLoss: 0.120711\n",
      "Test set: Accuracy: 0.897851800249  /  Patient: 4\n",
      "\n",
      "Train Epoch: 7 [0/40479 (0%)]\tLoss: 0.111970\n",
      "Test set: Accuracy: 0.898443725014  /  Patient: 3\n",
      "\n",
      "Train Epoch: 8 [0/40479 (0%)]\tLoss: 0.114121\n",
      "Test set: Accuracy: 0.900495397179  /  Patient: 3\n",
      "\n",
      "Train Epoch: 9 [0/40479 (0%)]\tLoss: 0.113983\n",
      "Test set: Accuracy: 0.901198843965  /  Patient: 3\n",
      "\n",
      "Train Epoch: 10 [0/40479 (0%)]\tLoss: 0.136871\n",
      "Test set: Accuracy: 0.900285035151  /  Patient: 2\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
