{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from math import floor\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "import os\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_data = pd.read_csv(\"train_v2.csv\")\n",
    "#data = np.array(np.zeros((500,256,256,3)),dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def DataLoader(data,batch_size = 10):\n",
    "#    i = 0\n",
    "#    ret_data = []\n",
    "#    while True:\n",
    "#        \n",
    "#        if i+batch_size > images_names.shape[0]:\n",
    "#            get_data = images_names[i:]\n",
    "#            i=0\n",
    "#        else:\n",
    "#            get_data = images_names[i:i+batch_size]\n",
    "#            i += batch_size\n",
    "#            \n",
    "#        for image in get_data:\n",
    "#            ret_data.append(plt.imread(\"train-jpg/%s.jpg\" % image)[:,:,0:3].reshape(3,256,256))#\n",
    "#\n",
    "#        \n",
    "#        yield torch.from_numpy(np.array(ret_data).astype('float32')).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_valid_split(dataframe, test_size = 0.25, shuffle = False, random_seed = 0):\n",
    "    \"\"\" Return a list of splitted indices from a DataSet.\n",
    "    Indices can be used with DataLoader to build a train and validation set.\n",
    "    \n",
    "    Arguments:\n",
    "        A Dataframe\n",
    "        A test_size, as a float between 0 and 1 (percentage split) or as an int (fixed number split)\n",
    "        Shuffling True or False\n",
    "        Random seed\n",
    "    \"\"\"\n",
    "    length = len(dataframe)\n",
    "    indices = list(range(1,length))\n",
    "    \n",
    "    if shuffle == True:\n",
    "        random.seed(random_seed)\n",
    "        random.shuffle(indices)\n",
    "    \n",
    "    if type(test_size) is float:\n",
    "        split = floor(test_size * length)\n",
    "    elif type(test_size) is int:\n",
    "        split = test_size\n",
    "    else:\n",
    "        raise ValueError('%s should be an int or a float' % str)\n",
    "    return indices[split:], indices[:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KaggleAmazonDataset(Dataset):\n",
    "    \"\"\"Dataset wrapping images and target labels for Kaggle - Planet Amazon from Space competition.\n",
    "\n",
    "    Arguments:\n",
    "        A CSV file path\n",
    "        Path to image folder\n",
    "        Extension of images\n",
    "        PIL transforms\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_path, img_path, img_ext, transform=None):\n",
    "    \n",
    "        tmp_df = pd.read_csv(csv_path)\n",
    "        assert tmp_df['image_name'].apply(lambda x: os.path.isfile(img_path + x + img_ext)).all(), \\\n",
    "\"Some images referenced in the CSV file were not found\"\n",
    "        \n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "        self.img_path = img_path\n",
    "        self.img_ext = img_ext\n",
    "        self.transform = transform\n",
    "\n",
    "        self.X_train = tmp_df['image_name']\n",
    "        self.y_train = self.mlb.fit_transform(tmp_df['tags'].str.split()).astype(np.float32)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.img_path + self.X_train[index] + self.img_ext)\n",
    "        img = img.convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img).cuda()\n",
    "        \n",
    "        label = torch.from_numpy(self.y_train[index]).cuda()\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = DataLoader(csv_data[\"image_name\"])\n",
    "#label = DataLoader(csv_data[\"tags\"])\n",
    "#for i,data in enumerate(data):\n",
    "    #print(i),print(data)\n",
    "IMG_PATH = 'train-jpg/'\n",
    "IMG_EXT = '.jpg'\n",
    "TRAIN_DATA = 'train_v2.csv'\n",
    "#transformations = transforms.Compose([transforms.Scale(32),transforms.ToTensor()])\n",
    "#dset_train = KaggleAmazonDataset(TRAIN_DATA,IMG_PATH,IMG_EXT,transformations)\n",
    "\n",
    "\n",
    "## Augmentation + Normalization for full training\n",
    "ds_transform_augmented = transforms.Compose([\n",
    "                 transforms.RandomSizedCrop(224),\n",
    "                 #transforms.Scale(512),\n",
    "                 transforms.RandomHorizontalFlip(), \n",
    "                 transforms.ToTensor(),\n",
    "                 #transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                # std=[0.229, 0.224, 0.225]),\n",
    "\n",
    "                 # Affine(\n",
    "                 #     rotation_range = 15,\n",
    "                 #     translation_range = (0.2,0.2),\n",
    "                 #     shear_range = math.pi/6,\n",
    "                 #     zoom_range=(0.7,1.4)\n",
    "                 # )\n",
    "                ])\n",
    "\n",
    "## Normalization only for validation and test\n",
    "ds_transform_raw = transforms.Compose([\n",
    "                 transforms.Scale(224),\n",
    "                 transforms.ToTensor(),\n",
    "                 transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.02, 0.02, 0.225])\n",
    "                 ])\n",
    "\n",
    "####     #########     ########     ###########     #####\n",
    "\n",
    "X_train = KaggleAmazonDataset(TRAIN_DATA,IMG_PATH,IMG_EXT,\n",
    "                             ds_transform_augmented\n",
    "                             )\n",
    "X_val = KaggleAmazonDataset(TRAIN_DATA,IMG_PATH,IMG_EXT,\n",
    "                             ds_transform_raw\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a validation split\n",
    "train_idx, valid_idx = train_valid_split(X_train, 0.2)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = DataLoader(X_train,\n",
    "                          batch_size=2,\n",
    "                          sampler=train_sampler,\n",
    "                          #num_workers=4,\n",
    "                          #pin_memory= True\n",
    "                         )\n",
    "\n",
    "valid_loader = DataLoader(X_val,\n",
    "                          batch_size=2,\n",
    "                          sampler=valid_sampler,\n",
    "                          #num_workers=4,\n",
    "                         #pin_memory= True\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_avgmax_pool2d(x, pool_type='avg', padding=0, count_include_pad=False):\n",
    "    \"\"\"Selectable global pooling function with dynamic input kernel size\n",
    "    \"\"\"\n",
    "    if pool_type == 'avgmaxc':\n",
    "        x = torch.cat([\n",
    "            F.avg_pool2d(\n",
    "                x, kernel_size=(x.size(2), x.size(3)), padding=padding, count_include_pad=count_include_pad),\n",
    "            F.max_pool2d(x, kernel_size=(x.size(2), x.size(3)), padding=padding)\n",
    "        ], dim=1)\n",
    "    elif pool_type == 'avgmax':\n",
    "        x_avg = F.avg_pool2d(\n",
    "                x, kernel_size=(x.size(2), x.size(3)), padding=padding, count_include_pad=count_include_pad)\n",
    "        x_max = F.max_pool2d(x, kernel_size=(x.size(2), x.size(3)), padding=padding)\n",
    "        x = 0.5 * (x_avg + x_max)\n",
    "    elif pool_type == 'max':\n",
    "        x = F.max_pool2d(x, kernel_size=(x.size(2), x.size(3)), padding=padding)\n",
    "    else:\n",
    "        if pool_type != 'avg':\n",
    "            print('Invalid pool type %s specified. Defaulting to average pooling.' % pool_type)\n",
    "        x = F.avg_pool2d(\n",
    "            x, kernel_size=(x.size(2), x.size(3)), padding=padding, count_include_pad=count_include_pad)\n",
    "    return x\n",
    "\n",
    "def pooling_factor(pool_type='avg'):\n",
    "    return 2 if pool_type == 'avgmaxc' else 1\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152']\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(\n",
    "        in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, drop_rate=0.0):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        self.drop_rate = drop_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        if self.drop_rate > 0.:\n",
    "            out = F.dropout(out, p=self.drop_rate, training=self.training)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, drop_rate=0.0):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        self.drop_rate = drop_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        if self.drop_rate > 0.:\n",
    "            out = F.dropout(out, p=self.drop_rate, training=self.training)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000,\n",
    "                 drop_rate=0.0, block_drop_rate=0.0,\n",
    "                 global_pool='avg'):\n",
    "        self.inplanes = 64\n",
    "        self.drop_rate = drop_rate\n",
    "        self.expansion = block.expansion\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0], drop_rate=block_drop_rate)\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, drop_rate=block_drop_rate)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, drop_rate=block_drop_rate)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, drop_rate=block_drop_rate)\n",
    "        self.global_pool = AdaptiveAvgMaxPool2d(pool_type=global_pool)\n",
    "        self.fc = nn.Linear(512 * block.expansion * self.global_pool.factor(), num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, drop_rate=0.):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = [block(self.inplanes, planes, stride, downsample, drop_rate)]\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def get_fc(self):\n",
    "        return self.fc\n",
    "\n",
    "    def reset_fc(self, num_classes, global_pool='avg'):\n",
    "        self.global_pool = AdaptiveAvgMaxPool2d(pool_type=global_pool)\n",
    "        self.fc = nn.Linear(512 * self.expansion * self.global_pool.factor(), num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        if self.drop_rate > 0.:\n",
    "            x = F.dropout(x, p=self.drop_rate, training=self.training)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def resnet18(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet34(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet34']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet50(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet101(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet101']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet152(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-152 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveAvgMaxPool2d(torch.nn.Module):\n",
    "    \"\"\"Selectable global pooling layer with dynamic input kernel size\n",
    "    \"\"\"\n",
    "    def __init__(self, output_size=1, pool_type='avg'):\n",
    "        super(AdaptiveAvgMaxPool2d, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.pool_type = pool_type\n",
    "        if pool_type == 'avgmaxc' or pool_type == 'avgmax':\n",
    "            self.pool = nn.ModuleList([nn.AdaptiveAvgPool2d(output_size), nn.AdaptiveMaxPool2d(output_size)])\n",
    "        elif pool_type == 'max':\n",
    "            self.pool = nn.AdaptiveMaxPool2d(output_size)\n",
    "        else:\n",
    "            if pool_type != 'avg':\n",
    "                print('Invalid pool type %s specified. Defaulting to average pooling.' % pool_type)\n",
    "            self.pool = nn.AdaptiveAvgPool2d(output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.pool_type == 'avgmaxc':\n",
    "            x = torch.cat([p(x) for p in self.pool], dim=1)\n",
    "        elif self.pool_type == 'avgmax':\n",
    "            x = 0.5 * torch.sum(torch.stack([p(x) for p in self.pool]), 0).squeeze(dim=0)\n",
    "        else:\n",
    "            x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "    def factor(self):\n",
    "        return pooling_factor(self.pool_type)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + 'output_size=' + str(self.output_size) \\\n",
    "+ ', pool_type=' + self.pool_type + ')'\n",
    "\n",
    "#model = Net()\n",
    "#model = models.inception_v3(num_classes=17, aux_logits=False, transform_input = True)\n",
    "#model = models.vgg16(pretrained=True, num_classes=17)\n",
    "model = resnet50(num_classes=17)\n",
    "optimizer = optim.RMSprop(model.parameters(),lr=0.00001)\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.0001)\n",
    "criterion = nn.MultiLabelSoftMarginLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    return fbeta_score(output, target, beta=2, average=\"samples\")\n",
    "\n",
    "def train_generator(model,epoch,train_loader,optimizer,criterion):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            #data, target = data.cuda(async=True), target.cuda(async=True) # On GPU\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 50 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "                \n",
    "def test_generator(model, val_loader, criterion):\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        ammount = 0\n",
    "        equal = 0\n",
    "        pred_all = None\n",
    "        test_all = None\n",
    "        test = 0\n",
    "        for data, target in val_loader:\n",
    "            data, target = Variable(data.float(), volatile=True), Variable(target.float())\n",
    "            output = model(data)\n",
    "            ammount += len(data)\n",
    "            test_loss += criterion(output, target).data[0]\n",
    "            pred = output\n",
    "            pred[pred > 0.5] = 1\n",
    "            pred[pred != 1] = 0\n",
    "\n",
    "            if type(pred_all) == type(None):\n",
    "                pred_all = pred.cpu().data.numpy().astype(np.uint8)\n",
    "            else:\n",
    "                pred = pred.cpu().data.numpy().astype(np.uint8)\n",
    "\n",
    "                pred_all = np.concatenate((pred_all,pred),axis=0)\n",
    "            if type(test_all) == type(None):\n",
    "                test_all = target.cpu().data.numpy().astype(np.uint8)\n",
    "            else:\n",
    "                target = target.cpu().data.numpy().astype(np.uint8)\n",
    "                test_all = np.concatenate((test_all,target),axis=0)\n",
    "            test += 1\n",
    "            #accuracy(output, target,topk=(1,3))\n",
    "            if test > 10:\n",
    "                break\n",
    "        test_loss /= ammount\n",
    "        print('Test set: Accuracy: %s\\n' % accuracy(pred_all,test_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/40479 (0%)]\tLoss: 0.302637\n",
      "Train Epoch: 1 [100/40479 (0%)]\tLoss: 0.249423\n",
      "Train Epoch: 1 [200/40479 (1%)]\tLoss: 0.190985\n",
      "Train Epoch: 1 [300/40479 (1%)]\tLoss: 0.403225\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.DataParallel(model).cuda()\n",
    "for epoch in range(1, 10 + 1):\n",
    "    train_generator(model,epoch,train_loader,optimizer,criterion)\n",
    "    test_generator(model,valid_loader,criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
