{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from math import floor\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "import os\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = pd.read_csv(\"train_v2.csv\")\n",
    "#data = np.array(np.zeros((500,256,256,3)),dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def DataLoader(data,batch_size = 10):\n",
    "#    i = 0\n",
    "#    ret_data = []\n",
    "#    while True:\n",
    "#        \n",
    "#        if i+batch_size > images_names.shape[0]:\n",
    "#            get_data = images_names[i:]\n",
    "#            i=0\n",
    "#        else:\n",
    "#            get_data = images_names[i:i+batch_size]\n",
    "#            i += batch_size\n",
    "#            \n",
    "#        for image in get_data:\n",
    "#            ret_data.append(plt.imread(\"train-jpg/%s.jpg\" % image)[:,:,0:3].reshape(3,256,256))#\n",
    "#\n",
    "#        \n",
    "#        yield torch.from_numpy(np.array(ret_data).astype('float32')).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_split(dataframe, test_size = 0.25, shuffle = False, random_seed = 0):\n",
    "    \"\"\" Return a list of splitted indices from a DataSet.\n",
    "    Indices can be used with DataLoader to build a train and validation set.\n",
    "    \n",
    "    Arguments:\n",
    "        A Dataframe\n",
    "        A test_size, as a float between 0 and 1 (percentage split) or as an int (fixed number split)\n",
    "        Shuffling True or False\n",
    "        Random seed\n",
    "    \"\"\"\n",
    "    length = len(dataframe)\n",
    "    indices = list(range(1,length))\n",
    "    \n",
    "    if shuffle == True:\n",
    "        random.seed(random_seed)\n",
    "        random.shuffle(indices)\n",
    "    \n",
    "    if type(test_size) is float:\n",
    "        split = floor(test_size * length)\n",
    "    elif type(test_size) is int:\n",
    "        split = test_size\n",
    "    else:\n",
    "        raise ValueError('%s should be an int or a float' % str)\n",
    "    return indices[split:], indices[:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KaggleAmazonDataset(Dataset):\n",
    "    \"\"\"Dataset wrapping images and target labels for Kaggle - Planet Amazon from Space competition.\n",
    "\n",
    "    Arguments:\n",
    "        A CSV file path\n",
    "        Path to image folder\n",
    "        Extension of images\n",
    "        PIL transforms\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_path, img_path, img_ext, transform=None):\n",
    "    \n",
    "        tmp_df = pd.read_csv(csv_path)\n",
    "        assert tmp_df['image_name'].apply(lambda x: os.path.isfile(img_path + x + img_ext)).all(), \\\n",
    "\"Some images referenced in the CSV file were not found\"\n",
    "        \n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "        self.img_path = img_path\n",
    "        self.img_ext = img_ext\n",
    "        self.transform = transform\n",
    "\n",
    "        self.X_train = tmp_df['image_name']\n",
    "        self.y_train = self.mlb.fit_transform(tmp_df['tags'].str.split()).astype(np.float32)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.img_path + self.X_train[index] + self.img_ext)\n",
    "        img = img.convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        label = torch.from_numpy(self.y_train[index])\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = DataLoader(csv_data[\"image_name\"])\n",
    "#label = DataLoader(csv_data[\"tags\"])\n",
    "#for i,data in enumerate(data):\n",
    "    #print(i),print(data)\n",
    "IMG_PATH = 'train-jpg/'\n",
    "IMG_EXT = '.jpg'\n",
    "TRAIN_DATA = 'train_v2.csv'\n",
    "#transformations = transforms.Compose([transforms.Scale(32),transforms.ToTensor()])\n",
    "#dset_train = KaggleAmazonDataset(TRAIN_DATA,IMG_PATH,IMG_EXT,transformations)\n",
    "\n",
    "\n",
    "## Augmentation + Normalization for full training\n",
    "ds_transform_augmented = transforms.Compose([\n",
    "                 #transforms.RandomSizedCrop(224),\n",
    "                 transforms.Scale(512),\n",
    "                 transforms.RandomHorizontalFlip(),\n",
    "                 #transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                # std=[0.229, 0.224, 0.225]),\n",
    "                 transforms.ToTensor(),\n",
    "                 #\n",
    "\n",
    "                 # Affine(\n",
    "                 #     rotation_range = 15,\n",
    "                 #     translation_range = (0.2,0.2),\n",
    "                 #     shear_range = math.pi/6,\n",
    "                 #     zoom_range=(0.7,1.4)\n",
    "                 # )\n",
    "])\n",
    "\n",
    "## Normalization only for validation and test\n",
    "ds_transform_raw = transforms.Compose([\n",
    "                 transforms.Scale(224),\n",
    "                 transforms.ToTensor(),\n",
    "                 transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "                 ])\n",
    "\n",
    "####     #########     ########     ###########     #####\n",
    "\n",
    "X_train = KaggleAmazonDataset(TRAIN_DATA,IMG_PATH,IMG_EXT,\n",
    "                             ds_transform_augmented\n",
    "                             )\n",
    "X_val = KaggleAmazonDataset(TRAIN_DATA,IMG_PATH,IMG_EXT,\n",
    "                             ds_transform_raw\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a validation split\n",
    "train_idx, valid_idx = train_valid_split(X_train, 0.2)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = DataLoader(X_train,\n",
    "                          batch_size=2,\n",
    "                          sampler=train_sampler,\n",
    "                          num_workers=4,\n",
    "                         )\n",
    "\n",
    "valid_loader = DataLoader(X_val,\n",
    "                          batch_size=2,\n",
    "                          sampler=valid_sampler,\n",
    "                          num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(186624, 256)\n",
    "        self.fc2 = nn.Linear(256, 17)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(x.size(0), -1) # Flatten layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.sigmoid(x)\n",
    "    \n",
    "    def train_generator(model,epoch,train_loader,optimizer,loss):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # data, target = data.cuda(async=True), target.cuda(async=True) # On GPU\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.binary_cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 10 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "                \n",
    "    def test_generator(model, val_loader, loss):\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        ammount = 0\n",
    "        for data, target in val_loader:\n",
    "            data, target = Variable(data.float(), volatile=True), Variable(target.float())\n",
    "            output = model(data)\n",
    "            ammount += len(data)\n",
    "            test_loss += loss(output, target).data[0]\n",
    "\n",
    "        test_loss /= ammount\n",
    "        print('Test set: Average loss: %s\\n' % test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model = models.inception_v3(num_classes=17, aux_logits=False)\n",
    "optimizer = optim.RMSprop(model.parameters(),lr=0.00001)\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(model,epoch,train_loader,optimizer,loss):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # data, target = data.cuda(async=True), target.cuda(async=True) # On GPU\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.binary_cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 10 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "                \n",
    "def test_generator(model, val_loader, loss):\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        ammount = 0\n",
    "        for data, target in val_loader:\n",
    "            data, target = Variable(data.float(), volatile=True), Variable(target.float())\n",
    "            output = model(data)\n",
    "            ammount += len(data)\n",
    "            test_loss += loss(output, target).data[0]\n",
    "\n",
    "        test_loss /= ammount\n",
    "        print('Test set: Average loss: %s\\n' % test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/40479 (0%)]\tLoss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/diego/.conda/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-235-9c9534378e9a>\", line 2, in <module>\n",
      "    train_generator(model,epoch,train_loader,optimizer,MSELoss)\n",
      "  File \"<ipython-input-234-2571e44fe250>\", line 7, in train_generator\n",
      "    output = model(data)\n",
      "  File \"/home/diego/.conda/envs/tensorflow/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 206, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/diego/.conda/envs/tensorflow/lib/python3.6/site-packages/torchvision/models/inception.py\", line 114, in forward\n",
      "    x = self.Mixed_7c(x)\n",
      "  File \"/home/diego/.conda/envs/tensorflow/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 206, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/diego/.conda/envs/tensorflow/lib/python3.6/site-packages/torchvision/models/inception.py\", line 277, in forward\n",
      "    branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
      "  File \"/home/diego/.conda/envs/tensorflow/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 206, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/diego/.conda/envs/tensorflow/lib/python3.6/site-packages/torchvision/models/inception.py\", line 324, in forward\n",
      "    x = self.conv(x)\n",
      "  File \"/home/diego/.conda/envs/tensorflow/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 206, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/diego/.conda/envs/tensorflow/lib/python3.6/site-packages/torch/nn/modules/conv.py\", line 237, in forward\n",
      "    self.padding, self.dilation, self.groups)\n",
      "  File \"/home/diego/.conda/envs/tensorflow/lib/python3.6/site-packages/torch/nn/functional.py\", line 40, in conv2d\n",
      "    return f(input, weight, bias)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/diego/.conda/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1828, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/diego/.conda/envs/tensorflow/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1090, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/diego/.conda/envs/tensorflow/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/diego/.conda/envs/tensorflow/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/diego/.conda/envs/tensorflow/lib/python3.6/inspect.py\", line 1480, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/diego/.conda/envs/tensorflow/lib/python3.6/inspect.py\", line 1438, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/diego/.conda/envs/tensorflow/lib/python3.6/inspect.py\", line 693, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/diego/.conda/envs/tensorflow/lib/python3.6/inspect.py\", line 729, in getmodule\n",
      "    for modname, module in list(sys.modules.items()):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 1 + 1):\n",
    "    train_generator(model,epoch,train_loader,optimizer,MSELoss)\n",
    "    #test_generator(model,valid_loader,MSELoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
