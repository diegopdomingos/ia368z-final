{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from math import floor\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "import os\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_data = pd.read_csv(\"train_v2.csv\")\n",
    "#data = np.array(np.zeros((500,256,256,3)),dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def DataLoader(data,batch_size = 10):\n",
    "#    i = 0\n",
    "#    ret_data = []\n",
    "#    while True:\n",
    "#        \n",
    "#        if i+batch_size > images_names.shape[0]:\n",
    "#            get_data = images_names[i:]\n",
    "#            i=0\n",
    "#        else:\n",
    "#            get_data = images_names[i:i+batch_size]\n",
    "#            i += batch_size\n",
    "#            \n",
    "#        for image in get_data:\n",
    "#            ret_data.append(plt.imread(\"train-jpg/%s.jpg\" % image)[:,:,0:3].reshape(3,256,256))#\n",
    "#\n",
    "#        \n",
    "#        yield torch.from_numpy(np.array(ret_data).astype('float32')).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_valid_split(dataframe, test_size = 0.25, shuffle = False, random_seed = 0):\n",
    "    \"\"\" Return a list of splitted indices from a DataSet.\n",
    "    Indices can be used with DataLoader to build a train and validation set.\n",
    "    \n",
    "    Arguments:\n",
    "        A Dataframe\n",
    "        A test_size, as a float between 0 and 1 (percentage split) or as an int (fixed number split)\n",
    "        Shuffling True or False\n",
    "        Random seed\n",
    "    \"\"\"\n",
    "    length = len(dataframe)\n",
    "    indices = list(range(1,length))\n",
    "    \n",
    "    if shuffle == True:\n",
    "        random.seed(random_seed)\n",
    "        random.shuffle(indices)\n",
    "    \n",
    "    if type(test_size) is float:\n",
    "        split = floor(test_size * length)\n",
    "    elif type(test_size) is int:\n",
    "        split = test_size\n",
    "    else:\n",
    "        raise ValueError('%s should be an int or a float' % str)\n",
    "    return indices[split:], indices[:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KaggleAmazonDataset(Dataset):\n",
    "    \"\"\"Dataset wrapping images and target labels for Kaggle - Planet Amazon from Space competition.\n",
    "\n",
    "    Arguments:\n",
    "        A CSV file path\n",
    "        Path to image folder\n",
    "        Extension of images\n",
    "        PIL transforms\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_path, img_path, img_ext, transform=None):\n",
    "    \n",
    "        tmp_df = pd.read_csv(csv_path)\n",
    "        assert tmp_df['image_name'].apply(lambda x: os.path.isfile(img_path + x + img_ext)).all(), \\\n",
    "\"Some images referenced in the CSV file were not found\"\n",
    "        \n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "        self.img_path = img_path\n",
    "        self.img_ext = img_ext\n",
    "        self.transform = transform\n",
    "\n",
    "        self.X_train = tmp_df['image_name']\n",
    "        self.y_train = self.mlb.fit_transform(tmp_df['tags'].str.split()).astype(np.float32)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.img_path + self.X_train[index] + self.img_ext)\n",
    "        img = img.convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        label = torch.from_numpy(self.y_train[index])\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = DataLoader(csv_data[\"image_name\"])\n",
    "#label = DataLoader(csv_data[\"tags\"])\n",
    "#for i,data in enumerate(data):\n",
    "    #print(i),print(data)\n",
    "IMG_PATH = 'train-jpg/'\n",
    "IMG_EXT = '.jpg'\n",
    "TRAIN_DATA = 'train_v2.csv'\n",
    "#transformations = transforms.Compose([transforms.Scale(32),transforms.ToTensor()])\n",
    "#dset_train = KaggleAmazonDataset(TRAIN_DATA,IMG_PATH,IMG_EXT,transformations)\n",
    "\n",
    "\n",
    "## Augmentation + Normalization for full training\n",
    "ds_transform_augmented = transforms.Compose([\n",
    "                 #transforms.RandomSizedCrop(224),\n",
    "                 transforms.Scale(512),\n",
    "                 transforms.RandomHorizontalFlip(), \n",
    "                 transforms.ToTensor(),\n",
    "                 #transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                # std=[0.229, 0.224, 0.225]),\n",
    "\n",
    "                 # Affine(\n",
    "                 #     rotation_range = 15,\n",
    "                 #     translation_range = (0.2,0.2),\n",
    "                 #     shear_range = math.pi/6,\n",
    "                 #     zoom_range=(0.7,1.4)\n",
    "                 # )\n",
    "                ])\n",
    "\n",
    "## Normalization only for validation and test\n",
    "ds_transform_raw = transforms.Compose([\n",
    "                 transforms.Scale(224),\n",
    "                 transforms.ToTensor(),\n",
    "                 transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.02, 0.02, 0.225])\n",
    "                 ])\n",
    "\n",
    "####     #########     ########     ###########     #####\n",
    "\n",
    "X_train = KaggleAmazonDataset(TRAIN_DATA,IMG_PATH,IMG_EXT,\n",
    "                             ds_transform_augmented\n",
    "                             )\n",
    "X_val = KaggleAmazonDataset(TRAIN_DATA,IMG_PATH,IMG_EXT,\n",
    "                             ds_transform_raw\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating a validation split\n",
    "train_idx, valid_idx = train_valid_split(X_train, 0.2)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = DataLoader(X_train,\n",
    "                          batch_size=2,\n",
    "                          sampler=train_sampler,\n",
    "                          num_workers=4,\n",
    "                         )\n",
    "\n",
    "valid_loader = DataLoader(X_val,\n",
    "                          batch_size=2,\n",
    "                          sampler=valid_sampler,\n",
    "                          num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(186624, 256)\n",
    "        self.fc2 = nn.Linear(256, 17)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(x.size(0), -1) # Flatten layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.sigmoid(x)\n",
    "    \n",
    "    def train_generator(model,epoch,train_loader,optimizer,loss):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # data, target = data.cuda(async=True), target.cuda(async=True) # On GPU\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.binary_cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 10 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "                \n",
    "    def test_generator(model, val_loader, loss):\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        ammount = 0\n",
    "        for data, target in val_loader:\n",
    "            data, target = Variable(data.float(), volatile=True), Variable(target.float())\n",
    "            output = model(data)\n",
    "            ammount += len(data)\n",
    "            test_loss += loss(output, target).data[0]\n",
    "\n",
    "        test_loss /= ammount\n",
    "        print('Test set: Average loss: %s\\n' % test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/torchvision-0.1.9-py3.5.egg/torchvision/models/inception.py:65: UserWarning: src is not broadcastable to dst, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "model = models.inception_v3(num_classes=17, aux_logits=False, transform_input = True)\n",
    "optimizer = optim.RMSprop(model.parameters(),lr=0.00001)\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(model,epoch,train_loader,optimizer):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # data, target = data.cuda(async=True), target.cuda(async=True) # On GPU\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            print(data[0].max())\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.binary_cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 10 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "                \n",
    "def test_generator(model, val_loader, loss):\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        ammount = 0\n",
    "        for data, target in val_loader:\n",
    "            data, target = Variable(data.float(), volatile=True), Variable(target.float())\n",
    "            output = model(data)\n",
    "            ammount += len(data)\n",
    "            test_loss += loss(output, target).data[0]\n",
    "\n",
    "        test_loss /= ammount\n",
    "        print('Test set: Average loss: %s\\n' % test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.3137\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Assertion `x >= 0. && x <= 1.' failed. input value should be between 0~1, but got -3.267984 at /opt/conda/conda-bld/pytorch_1503968623488/work/torch/lib/THNN/generic/BCECriterion.c:34",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-20d8471daa20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m#test_generator(model,valid_loader,MSELoss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-8ae2ff199c35>\u001b[0m in \u001b[0;36mtrain_generator\u001b[0;34m(model, epoch, train_loader, optimizer)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average)\u001b[0m\n\u001b[1;32m    774\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/torch/nn/_functions/thnn/auto.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, input, target, *args)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         getattr(ctx._backend, update_output.name)(ctx._backend.library_state, input, target,\n\u001b[0;32m---> 47\u001b[0;31m                                                   output, *ctx.additional_args)\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Assertion `x >= 0. && x <= 1.' failed. input value should be between 0~1, but got -3.267984 at /opt/conda/conda-bld/pytorch_1503968623488/work/torch/lib/THNN/generic/BCECriterion.c:34"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 1 + 1):\n",
    "    train_generator(model,epoch,train_loader,optimizer)\n",
    "    #test_generator(model,valid_loader,MSELoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
